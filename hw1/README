## This is the readme file for generatedata.py and processdata.py written for CME211 - Homework 1
## by Mert Can Simsek. The output logs for 3 examples and the answers to the questions provided in 
## the homework are presented.

# ____________ Part 1 _____________

## Output logs:

$ python3 generatedata.py 1000 600 50 "ref_1.txt" "reads_1.txt"
reference length: 1000
number reads: 600
read length: 50
aligns 0: 0.13333333333333333
aligns 1: 0.7683333333333333
aligns 2: 0.09833333333333333

$ python3 generatedata.py 10000 6000 50 "ref_2.txt" "reads_2.txt"
reference length: 10000
number reads: 6000
read length: 50
aligns 0: 0.1455
aligns 1: 0.7515
aligns 2: 0.103

$ python3 generatedata.py 100000 60000 50 "ref_3.txt" "reads_3.txt"
reference length: 100000
number reads: 60000
read length: 50
aligns 0: 0.15048333333333333
aligns 1: 0.74895
aligns 2: 0.10056666666666667

## Other than the reference and read lengths, the consideration I took into account while designing the
## handwritten data was to pick different places in the reference for the reads that align once so that
## it would help to catch a bug related with the placement of reads. Also, for the read which doesn't align,
## it was written in such a way that a part of it is contained in the reference.The aim was to see if the
## search is held correctly.

## We shouldn't expect an exact 15% / 75% / 10% distribution for the reads since it is determined by
## a number that is generated randomly. Also, the number of reads doesn't necessarily have to be a divisor 
## of 100. For small number of reads, the deviation from the desired distribution may be large, but as the
## number of reads generated increases, we expect the ratios to converge to the ratios desired, as can be
## seen in the example outputs above. Additionally, an alignment of a read overlap with another alignment
## of the same read, where we count it as 

## The total amount of time needed for this part to get correct results was around 3 hours, and the amount
## of time for adding the comments was around 20 minutes. Since this was my first Python experience,
## I spent some time looking up the basic syntax from lecture notes, but I expect as I get more used to
## coding in Python, the time I spend will be more and more efficient.

# ____________ Part 2 _____________

##Output logs:

python3 processdata.py ref_1.txt reads_1.txt align_1.txt

aligns 0: 0.13333333333333333
aligns 1: 0.7683333333333333
aligns 2: 0.09833333333333333

Time elapsed: 0.03677034378051758 seconds.

$ python3 processdata.py ref_2.txt reads_2.txt align_2.txt

aligns 0: 0.1455
aligns 1: 0.7515
aligns 2: 0.103

Time elapsed: 0.2770378589630127 seconds.

$ python3 processdata.py ref_3.txt reads_3.txt align_3.txt

aligns 0: 0.15048333333333333
aligns 1: 0.74895
aligns 2: 0.10056666666666667

Time elapsed: 24.05472755432129 seconds.

## The distribution of the reads match exactly to the distribution as the data was created.
## Since we specified a pattern of data when we were generating the reads, this result was expected.
## More than 2 alignments of a read is also counted as 2 (intentionally), since we ignore the third
## possible alignment, so an error is not introduced in that sense. However, even though we generate
## the data in a specified pattern, repetitions from the first half can be seen in the last 25%, leading
## to extra two aligned reads than intended. The chances of this is small given the large number of 
## read lengths, and this was seen only in a few cases of test runs. 

## Using the time module, elapsed time required for the data set 1 was around 0.037 seconds,
## for the data set 2, it increased to 0.277 seconds, and for the data set 3, it was 24.05 seconds.
## For the sake of a more accurate observation of the trend, I've generated a fourth data set with a
## reference length with 1000000 with 600000 reads with read length 50. Process time required for this set
## was 2384.94 seconds. So the trend clearly seems to be linear. For a coverage of 30x, as we increase
## the reference length and the amount of reads, the time required to process the data increases with the
## same rate. Thus, for a human (who typically has a reference length of 3 billion length) at 30x coverage
## and a read length of 50, the time required would be approximately 7154820 seconds, which is 1987.5
## hours. So using this program is not feasible for everyday practice and the algorithm needs to be
## optimized further. Also, as seen in the lectures, we can say that finding sub-strings are in the order
## of O(n) in time complexity. Linear increase of time complexity agrees with the results we achieved.

## This part of the code took less than 3 hours. Commenting and preparing the writeup took approximately
## half an hour.
